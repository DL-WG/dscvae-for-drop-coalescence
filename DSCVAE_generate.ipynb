{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"f9Ap0HdHhKL_"},"outputs":[],"source":["n_samples = 3285"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3580,"status":"ok","timestamp":1668368939177,"user":{"displayName":"Kewei Zhu","userId":"12743063082510108927"},"user_tz":0},"id":"F-uzBs_mTpyg","outputId":"9a357896-57c6-4b67-f32b-2d8b94eac244"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7fb1849e7470>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch import nn, optim\n","torch.manual_seed(3409)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E45KIKNXTrGc"},"outputs":[],"source":["import numpy as np\n","np.random.seed(0)\n","import pandas as pd\n","from pandas.core.frame import DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gz2L3NjjTr1U"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import seaborn as sns; sns.set_theme()\n","\n","sns.set_theme(style='white')\n","sns.set_palette(\"muted\")\n","\n","def change_width(ax, new_value) :\n","    for patch in ax.patches :\n","        current_width = patch.get_width()\n","        diff = current_width - new_value\n","\n","        # we change the bar width\n","        patch.set_width(new_value)\n","\n","        # we recenter the bar\n","        patch.set_x(patch.get_x() + diff * .5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18266,"status":"ok","timestamp":1668368958042,"user":{"displayName":"Kewei Zhu","userId":"12743063082510108927"},"user_tz":0},"id":"XPdLaII0TslQ","outputId":"31d9b03f-ea0b-4256-9d0a-ac0e8d2c5910"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnUuHJBHaZrF"},"outputs":[],"source":["import time\n","timestr = time.strftime(\"%Y%m%d-%H%M%S\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmAIh8h0TtqG"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABrwLrjUTvRB"},"outputs":[],"source":["train_balanced = pd.read_csv(\"/content/drive/MyDrive/20221108/dataset/train_balanced.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHmkatm8hh3z"},"outputs":[],"source":["import sys\n","sys.path.insert(0,'/content/drive/MyDrive/20221108')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-elIkZmjTzmA"},"outputs":[],"source":["import databuild"]},{"cell_type":"markdown","metadata":{"id":"c86ysnl_UAVm"},"source":["# Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":312,"status":"ok","timestamp":1668368959371,"user":{"displayName":"Kewei Zhu","userId":"12743063082510108927"},"user_tz":0},"id":"X7a-ZFZLT2ol","outputId":"1627c37e-a4f5-425b-f6eb-7571b48aacb8"},"outputs":[{"data":{"text/plain":["Autoencoder(\n","  (fc21): Linear(in_features=4, out_features=4, bias=True)\n","  (fc22): Linear(in_features=4, out_features=4, bias=True)\n","  (encoder): Sequential(\n","    (0): Linear(in_features=4, out_features=4, bias=True)\n","    (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Linear(in_features=4, out_features=4, bias=True)\n","    (4): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=4, out_features=4, bias=True)\n","    (7): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU()\n","    (9): Linear(in_features=4, out_features=4, bias=True)\n","    (10): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): ReLU()\n","    (12): Linear(in_features=4, out_features=4, bias=True)\n","    (13): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (14): ReLU()\n","  )\n","  (decoder): Sequential(\n","    (0): Linear(in_features=4, out_features=4, bias=True)\n","    (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Linear(in_features=4, out_features=4, bias=True)\n","    (4): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=4, out_features=4, bias=True)\n","    (7): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU()\n","    (9): Linear(in_features=4, out_features=4, bias=True)\n","    (10): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): ReLU()\n","    (12): Linear(in_features=4, out_features=4, bias=True)\n","    (13): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (14): ReLU()\n","  )\n","  (classifier1): Sequential(\n","    (0): Linear(in_features=4, out_features=4, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=4, out_features=4, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=4, out_features=2, bias=True)\n","    (5): Sigmoid()\n","  )\n","  (classifier2): Sequential(\n","    (0): Linear(in_features=4, out_features=4, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=4, out_features=4, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=4, out_features=2, bias=True)\n","    (5): Sigmoid()\n","  )\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["class Autoencoder(nn.Module):\n","  \n","    def __init__(self, D_in, H1, H2, H3, latent_dim): \n","      \n","        super(Autoencoder,self).__init__()\n","        \n","        self.fc21 = nn.Linear(latent_dim, latent_dim)\n","        self.fc22 = nn.Linear(latent_dim, latent_dim)\n","        \n","        self.encoder = nn.Sequential(\n","                nn.Linear(D_in, H1), nn.BatchNorm1d(H1), nn.ReLU(),\n","                nn.Linear(H1, H2),  nn.BatchNorm1d(H2), nn.ReLU(),\n","                nn.Linear(H2, H3),  nn.BatchNorm1d(H3), nn.ReLU(), \n","                nn.Linear(H3, latent_dim), nn.BatchNorm1d(latent_dim), nn.ReLU(),\n","                nn.Linear(latent_dim, latent_dim), nn.BatchNorm1d(latent_dim), nn.ReLU(),\n","                ) \n","\n","        self.decoder = nn.Sequential(\n","                nn.Linear(latent_dim, latent_dim), nn.BatchNorm1d(latent_dim), nn.ReLU(), \n","                nn.Linear(latent_dim, H3), nn.BatchNorm1d(H3), nn.ReLU(), \n","                nn.Linear(H3, H2),  nn.BatchNorm1d(H2),  nn.ReLU(), \n","                nn.Linear(H2, H1),  nn.BatchNorm1d(H1),  nn.ReLU(), \n","                nn.Linear(H1, D_in), nn.BatchNorm1d(D_in), nn.ReLU(),\n","                )\n","        \n","        self.classifier1 = nn.Sequential(\n","                nn.Linear(latent_dim, latent_dim), nn.ReLU(),\n","                nn.Linear(latent_dim, latent_dim), nn.ReLU(),\n","                nn.Linear(latent_dim, 2),   nn.Sigmoid(),\n","                )\n","        \n","        self.classifier2 = nn.Sequential(\n","                nn.Linear(D_in, D_in), nn.ReLU(),\n","                nn.Linear(D_in, D_in), nn.ReLU(),\n","                nn.Linear(D_in, 2),   nn.Sigmoid(),\n","                )\n","    \n","\n","    def forward(self, x): \n","        latent_fea = self.encoder(x)\n","        mu, logvar = self.compute_r1_r2(latent_fea)\n","        z = self.reparameterize(mu, logvar)\n","        generated_data = self.decoder(z)\n","        latent_label = self.classifier1(z)\n","        output_label = self.classifier2(generated_data) \n","        return  latent_fea, mu, logvar, z, generated_data, latent_label, output_label\n","      \n","    def compute_r1_r2(self, latent_fea):\n","        r1 = self.fc21(latent_fea)\n","        r2 = self.fc22(latent_fea)\n","        return r1, r2\n","\n","    def reparameterize(self, mu, logvar):\n","        if self.training:\n","            std = logvar.mul(0.5).exp_()\n","            eps = Variable(std.data.new(std.size()).normal_())\n","            return eps.mul(std).add_(mu)\n","        else:\n","            return mu\n","\n","D_in = 4\n","H1 = 4\n","H2 = 4\n","H3 = 4\n","latent_dim = 4\n","model = Autoencoder(D_in, H1, H2, H3, latent_dim)\n","\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/20221108/saved_model/dscvae.pth\", map_location='cpu'))\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1D-V-M9iBgt"},"outputs":[],"source":["learning_rate = 1e-3\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"bleQCtyqUE2m"},"source":["# Generate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfehANQrUJg-"},"outputs":[],"source":["def gaussian(dimension, n_sample):\n","  \n","    # Set mean vector. \n","    m = np.array([1, 1, 1 ,1]).reshape(4, 1)\n","    # Set covariance function. \n","    K_0 = np.array([[2, 1, 1, 1],\n","                    [1, 2, 1, 1],\n","                    [1, 1, 2, 1],\n","                    [1, 1, 1, 2]])\n","    # Eigenvalues covariance function.\n","    np.linalg.eigvals(K_0)\n","    # Define epsilon.\n","    epsilon = 0.0001\n","    # Add small pertturbation. \n","    K = K_0 + epsilon*np.identity(dimension)\n","    # Cholesky decomposition.\n","    L = np.linalg.cholesky(K)\n","    np.dot(L, np.transpose(L))\n","\n","    u = np.random.normal(loc = 0, scale = 1, size = dimension * n_sample).reshape(dimension, n_sample)\n","    x = m + np.dot(L, u)\n","    z = np.random.multivariate_normal(mean = m.reshape(dimension,), cov = K, size = n_sample)\n","    \n","    return z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7vwAue_UVf8"},"outputs":[],"source":["@torch.no_grad()\n","def generate_fake(model, z):\n","    pred = model.decoder(z).cpu().numpy()\n","    return pred"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"51VWWvOZUjWI"},"source":["## non-coalescence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1668368959598,"user":{"displayName":"Kewei Zhu","userId":"12743063082510108927"},"user_tz":0},"id":"nTL2rL7eUYx6","outputId":"1d60b13f-ca39-4877-beab-d3f2b76a9ad4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.5558, 0.9595, 0.7015, 0.8815]) tensor([1.5413, 1.2241, 0.9968, 1.0100])\n"]}],"source":["train_0 = train_balanced[train_balanced.iloc[: , -1] == 0]\n","train_0_X = train_0.iloc[:, 0:-1]\n","\n","generatedata0 = databuild.DataBuilder(train_0_X)\n","generateloader0 = databuild.DataLoader(dataset = generatedata0, batch_size = 219)\n","\n","with torch.no_grad():\n","\n","  for batch_idx, data in enumerate(generateloader0): \n","    data = data.to(device)\n","    optimizer.zero_grad()\n","    mu, logvar, latent_fea, z, latent_label, generated_data, output_label = model(data)\n","\n","input_data_tensor = torch.tensor(train_0_X.values.astype(np.float32)) \n","\n","sigma = torch.exp(logvar/2)\n","\n","# sample z from q\n","no_samples = n_samples\n","q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n","print(mu.mean(axis=0), sigma.mean(axis=0))\n","noise = gaussian(4, n_samples)\n","z = q.rsample(sample_shape=torch.Size([no_samples])) + 0.5 * torch.tensor(noise).float()\n","\n","generated_data = generate_fake(model, z)\n","cols = [\"chamber_size\",\"top\",\"bottom\",\"dt\"]\n","df_fake_0 = pd.DataFrame(generated_data, columns = cols) \n","\n","latent_fea_tensor = latent_fea.clone().detach().requires_grad_(True)\n","latent_fea = latent_fea_tensor.detach().numpy()\n","latent_fea_0 =pd.DataFrame(latent_fea)\n","\n","z_tensor = z.clone().detach().requires_grad_(True)\n","z = z_tensor.detach().numpy()\n","z_0 = pd.DataFrame(z)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Zk4r1IDoU_Si"},"source":["## coalescence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ro2zgfNqU_7f"},"outputs":[],"source":["train_1 = train_balanced[train_balanced.iloc[: , -1] == 1]\n","train_1_X = train_1.iloc[:, 0:-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7N3QDQxOVB4S"},"outputs":[],"source":["generatedata1 = databuild.DataBuilder(train_1_X)\n","generateloader1 = databuild.DataLoader(dataset = generatedata1, batch_size = 219)\n","\n","with torch.no_grad():\n","\n","  for batch_idx, data in enumerate(generateloader1):\n","    data = data.to(device)\n","    optimizer.zero_grad()\n","    mu, logvar, latent_fea, z, latent_label, generated_data, output_label = model(data)\n","\n","input_data_tensor = torch.tensor(train_1_X.values.astype(np.float32)) \n","\n","sigma = torch.exp(logvar/2)\n","\n","# sample z from q\n","no_samples = n_samples\n","q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n","noise = gaussian(4, n_samples)\n","z = q.rsample(sample_shape=torch.Size([no_samples])) + 0.5 * torch.tensor(noise).float()\n","\n","generated_data = generate_fake(model, z)\n","cols = [\"chamber_size\",\"top\",\"bottom\",\"dt\"]\n","df_fake_1 = pd.DataFrame(generated_data, columns = cols) \n","\n","latent_fea_tensor = latent_fea.clone().detach().requires_grad_(True)\n","latent_fea = latent_fea_tensor.detach().numpy()\n","latent_fea_1 =pd.DataFrame(latent_fea)\n","\n","z_tensor = z.clone().detach().requires_grad_(True)\n","z = z_tensor.detach().numpy()\n","z_1 = pd.DataFrame(z)"]},{"cell_type":"markdown","metadata":{"id":"MLwHftWOVNXO"},"source":["## Concat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4m0t0V2sVPiv"},"outputs":[],"source":["df_fake_0 = df_fake_0.iloc[:, 0:-1]\n","df_fake_0.insert(4, \"Coalescence\", 1.0)\n","df_fake_1 = df_fake_1.iloc[:, 0:-1]\n","df_fake_1.insert(4, \"Coalescence\", 0.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXABffg2VQ77"},"outputs":[],"source":["df_fake = pd.concat([df_fake_0, df_fake_1], axis = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7id_Az4VVUHY"},"outputs":[],"source":["df_fake.to_csv(\"/content/drive/MyDrive/20221108/dataset/fake_dscvae.csv\", index=False)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOeM5s9jTcrxNhgipnpIgOT","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
